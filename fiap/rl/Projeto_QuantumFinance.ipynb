{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrGy9ZUAHJ2-",
        "outputId": "6aaf6010-467e-46fd-d867-d903f357481b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando bibliotecas...\n",
            "Bibliotecas carregadas com sucesso.\n",
            "\n",
            "=========================================\n",
            "===      INICIANDO FASE DE TREINO     ===\n",
            "=========================================\n",
            "Baixando dados históricos de 2021-01-01 até 2024-12-31...\n",
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  3 of 3 completed\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados baixados com sucesso!\n",
            "\n",
            "--- Treino: Episódio 1/10 ---\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# PROJETO FINAL MBA IA & DS: QUANTUMFINANCE - AGENTE DE TRADING COM RL\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 1. IMPORTAÇÃO DAS BIBLIOTECAS ---\n",
        "print(\"Carregando bibliotecas...\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from collections import deque\n",
        "import random\n",
        "import itertools\n",
        "from datetime import date\n",
        "\n",
        "# Bibliotecas para a Rede Neural (DQN)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Biblioteca para visualização de dados\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "print(\"Bibliotecas carregadas com sucesso.\")\n",
        "\n",
        "# --- 2. DEFINIÇÃO DO AMBIENTE DE NEGOCIAÇÃO ---\n",
        "class TradingEnv:\n",
        "    \"\"\"\n",
        "    Classe que simula o ambiente da bolsa de valores para o agente de RL.\n",
        "    Aceita tickers e um período de tempo para carregar os dados.\n",
        "    \"\"\"\n",
        "    def __init__(self, tickers, start_date, end_date, initial_balance=10000, window_size=10):\n",
        "        self.tickers = tickers\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date\n",
        "        self.window_size = window_size\n",
        "        self.initial_balance = initial_balance\n",
        "        self.data = self._load_data()\n",
        "        self.action_map = self._create_action_map()\n",
        "        self.action_size = len(self.action_map)\n",
        "\n",
        "    def _load_data(self):\n",
        "        print(f\"Baixando dados históricos de {self.start_date} até {self.end_date}...\")\n",
        "        # yfinance agora ajusta o 'Close' automaticamente (auto_adjust=True por padrão)\n",
        "        full_data = yf.download(self.tickers, start=self.start_date, end=self.end_date)\n",
        "        if full_data.empty:\n",
        "            raise ValueError(\"Nenhum dado baixado. Verifique os tickers e o período.\")\n",
        "        df_close = full_data['Close']\n",
        "        print(\"Dados baixados com sucesso!\")\n",
        "        return df_close.dropna()\n",
        "\n",
        "    def _create_action_map(self):\n",
        "        # 0: Manter (Hold), 1: Comprar (Buy), 2: Vender (Sell)\n",
        "        actions_per_asset = [0, 1, 2]\n",
        "        action_combos = list(itertools.product(actions_per_asset, repeat=len(self.tickers)))\n",
        "        return {i: combo for i, combo in enumerate(action_combos)}\n",
        "\n",
        "    def _get_state(self, t):\n",
        "        state = [self.balance]\n",
        "        for ticker in self.tickers:\n",
        "            state.append(self.shares_held[ticker])\n",
        "\n",
        "        price_history = []\n",
        "        for ticker in self.tickers:\n",
        "            series = self.data[ticker].values\n",
        "            price_window = series[t - self.window_size : t]\n",
        "            normalized_window = price_window / series[t]\n",
        "            price_history.extend(normalized_window)\n",
        "\n",
        "        state.extend(price_history)\n",
        "        return np.array([state])\n",
        "\n",
        "    def reset(self):\n",
        "        self.balance = self.initial_balance\n",
        "        self.shares_held = {ticker: 0 for ticker in self.tickers}\n",
        "        self.current_step = self.window_size\n",
        "        self.portfolio_value_history = [self.initial_balance]\n",
        "        return self._get_state(self.current_step)\n",
        "\n",
        "    def step(self, action_index):\n",
        "        old_portfolio_value = sum(self.shares_held[ticker] * self.data[ticker].iloc[self.current_step] for ticker in self.tickers) + self.balance\n",
        "\n",
        "        action_tuple = self.action_map[action_index]\n",
        "        for i, ticker in enumerate(self.tickers):\n",
        "            action = action_tuple[i]\n",
        "            price = self.data[ticker].iloc[self.current_step]\n",
        "\n",
        "            if action == 1 and self.balance >= price: # Comprar\n",
        "                self.balance -= price\n",
        "                self.shares_held[ticker] += 1\n",
        "            elif action == 2 and self.shares_held[ticker] > 0: # Vender\n",
        "                self.balance += price\n",
        "                self.shares_held[ticker] -= 1\n",
        "\n",
        "        self.current_step += 1\n",
        "\n",
        "        new_portfolio_value = sum(self.shares_held[ticker] * self.data[ticker].iloc[self.current_step] for ticker in self.tickers) + self.balance\n",
        "        self.portfolio_value_history.append(new_portfolio_value)\n",
        "\n",
        "        reward = new_portfolio_value - old_portfolio_value\n",
        "        next_state = self._get_state(self.current_step)\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "# --- 3. DEFINIÇÃO DO AGENTE DQN ---\n",
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    O agente que aprende a operar no mercado usando uma Deep Q-Network.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential([\n",
        "            Dense(64, input_dim=self.state_size, activation='relu'),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                q_next = self.model.predict(next_state, verbose=0)[0]\n",
        "                target = reward + self.gamma * np.amax(q_next)\n",
        "\n",
        "            target_f = self.model.predict(state, verbose=0)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# --- 4. BLOCO DE EXECUÇÃO PRINCIPAL ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    tickers = [\"VALE3.SA\", \"PETR4.SA\", \"BRFS3.SA\"]\n",
        "\n",
        "    # --- FASE 1: TREINAMENTO ---\n",
        "    print(\"\\n=========================================\")\n",
        "    print(\"===      INICIANDO FASE DE TREINO     ===\")\n",
        "    print(\"=========================================\")\n",
        "\n",
        "    train_env = TradingEnv(tickers, start_date=\"2021-01-01\", end_date=\"2024-12-31\")\n",
        "\n",
        "    state_size = train_env.window_size * len(tickers) + 1 + len(tickers)\n",
        "    action_size = train_env.action_size\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "    episodes = 10\n",
        "    batch_size = 32\n",
        "\n",
        "    for e in range(episodes):\n",
        "        print(f\"\\n--- Treino: Episódio {e+1}/{episodes} ---\")\n",
        "        state = train_env.reset()\n",
        "        for time in range(train_env.window_size, len(train_env.data)):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = train_env.step(action)\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                print(f\"Episódio {e+1} CONCLUÍDO. Valor Final: {train_env.portfolio_value_history[-1]:.2f}\")\n",
        "                break\n",
        "            if len(agent.memory) > batch_size:\n",
        "                agent.replay(batch_size)\n",
        "\n",
        "    print(\"\\nTreinamento Finalizado. O agente está pronto para ser avaliado.\")\n",
        "\n",
        "    # --- FASE 2: AVALIAÇÃO (BACKTESTING) ---\n",
        "    print(\"\\n============================================\")\n",
        "    print(\"===   INICIANDO FASE DE AVALIAÇÃO (TESTE)  ===\")\n",
        "    print(\"============================================\")\n",
        "\n",
        "    today = date.today().strftime(\"%Y-%m-%d\")\n",
        "    test_env = TradingEnv(tickers, start_date=\"2025-01-01\", end_date=today)\n",
        "\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    state = test_env.reset()\n",
        "    for time in range(test_env.window_size, len(test_env.data)):\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = test_env.step(action)\n",
        "        state = next_state\n",
        "        if time % 20 == 0:\n",
        "            print(f\"  Avaliando... Dia {time}/{len(test_env.data)}, Valor do Portfólio: {test_env.portfolio_value_history[-1]:.2f}\")\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    final_test_value = test_env.portfolio_value_history[-1]\n",
        "    initial_test_value = test_env.initial_balance\n",
        "    test_profit = final_test_value - initial_test_value\n",
        "    test_return = (test_profit / initial_test_value) * 100\n",
        "\n",
        "    print(\"\\n--------------------------------------------\")\n",
        "    print(\"         RESULTADO DA AVALIAÇÃO\")\n",
        "    print(\"--------------------------------------------\")\n",
        "    print(f\"Capital Inicial: R$ {initial_test_value:.2f}\")\n",
        "    print(f\"Valor Final do Portfólio: R$ {final_test_value:.2f}\")\n",
        "    print(f\"Lucro/Prejuízo: R$ {test_profit:.2f}\")\n",
        "    print(f\"Rentabilidade no Período: {test_return:.2f}%\")\n",
        "    print(\"--------------------------------------------\")\n",
        "\n",
        "    # --- FASE 3: VISUALIZAÇÃO DOS RESULTADOS ---\n",
        "    print(\"\\n============================================\")\n",
        "    print(\"===   GERANDO GRÁFICO DE PERFORMANCE   ===\")\n",
        "    print(\"============================================\")\n",
        "\n",
        "    df_test = test_env.data\n",
        "    initial_balance = test_env.initial_balance\n",
        "    num_tickers = len(test_env.tickers)\n",
        "    capital_per_ticker = initial_balance / num_tickers\n",
        "    initial_prices = df_test.iloc[0]\n",
        "    shares_held = capital_per_ticker / initial_prices\n",
        "    benchmark_portfolio_value = df_test.mul(shares_held, axis='columns').sum(axis=1)\n",
        "\n",
        "    plot_df = pd.DataFrame(index=df_test.index[test_env.window_size-1:])\n",
        "    plot_df['Agente DQN'] = test_env.portfolio_value_history\n",
        "    benchmark_to_plot = benchmark_portfolio_value[test_env.window_size-1:].copy()\n",
        "    benchmark_to_plot = (benchmark_to_plot / benchmark_to_plot.iloc[0]) * initial_balance\n",
        "    plot_df['Benchmark (Buy and Hold)'] = benchmark_to_plot\n",
        "\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "    ax.plot(plot_df.index, plot_df['Agente DQN'], label='Agente DQN', color='royalblue', linewidth=2)\n",
        "    ax.plot(plot_df.index, plot_df['Benchmark (Buy and Hold)'], label='Benchmark (Buy and Hold)', color='gray', linestyle='--')\n",
        "\n",
        "    ax.set_title('Performance do Agente DQN vs. Benchmark (2025)', fontsize=16, pad=20)\n",
        "    ax.set_xlabel('Data', fontsize=12)\n",
        "    ax.set_ylabel('Valor do Portfólio (R$)', fontsize=12)\n",
        "\n",
        "    formatter = mticker.FormatStrFormatter('R$ %.2f')\n",
        "    ax.yaxis.set_major_formatter(formatter)\n",
        "\n",
        "    plt.xticks(rotation=45)\n",
        "    ax.legend(fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig('performance_agente_vs_benchmark.png', dpi=300)\n",
        "    print(\"\\nGráfico 'performance_agente_vs_benchmark.png' salvo com sucesso!\")\n",
        "\n",
        "    plt.show()"
      ]
    }
  ]
}