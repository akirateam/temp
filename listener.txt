Estamos continuando um automação em massa de Clusters ARO, e nos deparamos com os seguintes problemas:

- Durante a fase upgrade tivemos situações onde a atualização ficava pausada com problemas de fazer drain do nó em questão, normalmente isto ocorreu por causa de pods com pdbs, ou pods com problemas que ficavam em estado de Terminating.
- Ao final e conclusão do upgrade e hopes em Channel EUS, e tbem após o término dos MCPs de worker e infra atualizarem e finalizar por completo o upgrade, as vezes acontecem de pods de sistema ficarem em crashloopback por exemplo: csi driver de Azure file, e dynatrace

Pensando em upgrade em massa, fica inviável um atuação manual para estes casos que afetam um upgrade, pode identificar e corrigir além destes citados acima, demais problemas que impeçam este upgrade e estado saudável do cluster e micro serviços antes, durante e pós um upgrade.

A sugestão é como um listener, trigger ou Job no Red Hat AAP, a automação é toda em Ansible.

---

Este é um desafio clássico e crítico na automação de ambientes OpenShift em escala. A abordagem de "fire-and-forget" para upgrades não é sustentável, e a automação precisa ter inteligência para prever, identificar e corrigir problemas em tempo real.
A sua ideia de usar um "listener" ou "trigger" via AAP é exatamente o caminho certo. Vamos estruturar uma solução robusta usando Ansible, que pode ser implementada como um Job Template ou, de forma mais avançada, um Workflow no Red Hat AAP.
A estratégia se divide em três fases principais: Pré-Upgrade (Pre-Flight Checks), Durante o Upgrade (Monitoramento e Remediação Ativa), e Pós-Upgrade (Validação e Correção).
Estrutura da Automação Ansible no AAP
Você pode estruturar seu projeto Ansible com roles para cada fase, tornando-o modular e reutilizável.
ansible-project/
├── inventory/
│   └── aro_clusters.yml
├── roles/
│   ├── pre_upgrade_checks/
│   ├── execute_upgrade/
│   └── post_upgrade_validation/
├── playbooks/
│   ├── 01_pre_flight.yml
│   ├── 02_run_upgrade.yml
│   └── 03_post_flight.yml
└── upgrade_workflow.yml  (Playbook principal que chama os outros)

No AAP, isso se traduziria em um Workflow Template com três Job Templates, um para cada fase, encadeados com lógica de sucesso/falha.
Fase 1: Pré-Upgrade (Pre-Flight Health Checks)
Este é o passo mais importante para prevenir 90% dos problemas. O objetivo é garantir que o cluster esteja em um estado perfeitamente saudável antes de iniciar a atualização.
Problemas a Identificar e Corrigir:
 * Pods em Estado Anômalo (Terminating, CrashLoopBackOff, Error):
   * Identificação: Use o módulo k8s_info ou oc get pods para listar pods em namespaces críticos (openshift-*, kube-system, etc.) e em projetos de aplicação que não estejam no estado Running ou Completed.
   * Correção Automatizada:
     * Terminating: Estes pods geralmente estão presos por causa de finalizers. A correção é forçar a remoção patchando o pod para remover os finalizers e depois deletá-lo.
     - name: Encontrar pods presos em 'Terminating'
  k8s_info:
    api_version: v1
    kind: Pod
    namespace: "{{ item }}"
    field_selectors:
      - status.phase=Terminating
  register: terminating_pods
  loop: "{{ critical_namespaces }}"

- name: Forçar remoção de pods em 'Terminating'
  k8s:
    state: absent
    api_version: v1
    kind: Pod
    name: "{{ pod.metadata.name }}"
    namespace: "{{ pod.metadata.namespace }}"
    grace_period: 0
    force: yes
  loop: "{{ terminating_pods.resources | flatten(levels=1) }}"
  loop_control:
    loop_var: pod

     * CrashLoopBackOff / Error: Tentar uma deleção simples (k8s com state: absent). O Deployment/ReplicaSet irá recriá-lo. Se o problema persistir, o playbook deve falhar e alertar, pois pode ser um problema mais profundo.
 * PodDisruptionBudgets (PDBs) Restritivos:
   * Identificação: Liste todos os PDBs no cluster e verifique a configuração de minAvailable ou maxUnavailable. PDBs com minAvailable igual ao número de réplicas, ou maxUnavailable: 0, impedirão o drain do nó.
   * Correção Automatizada (com cautela):
     * Opção 1 (Segura): Apenas identificar e alertar, pausando a automação para intervenção manual.
     * Opção 2 (Agressiva): A automação pode temporariamente "relaxar" o PDB, por exemplo, alterando maxUnavailable de 0 para 1, realizar o upgrade, e depois restaurar o valor original. Isso requer armazenar o estado original do PDB antes da alteração.
     # Lógica para identificar e salvar o PDB original
# ...
- name: Relaxar PDB temporariamente para permitir o drain
  k8s:
    state: present
    definition:
      apiVersion: policy/v1
      kind: PodDisruptionBudget
      metadata:
        name: "{{ pdb_name }}"
        namespace: "{{ pdb_namespace }}"
      spec:
        maxUnavailable: 1 # ou um valor calculado
        selector: "{{ pdb_original.spec.selector }}"

 * Saúde dos Cluster Operators:
   * Identificação: Verifique se todos os ClusterOperators estão com AVAILABLE=True, PROGRESSING=False, e DEGRADED=False.
   * Correção: Geralmente, um Cluster Operator degradado exige análise manual. A automação deve falhar imediatamente se encontrar um operator em estado DEGRADED, pois iniciar um upgrade nessas condições é extremamente arriscado.
Fase 2: Durante o Upgrade (Monitoramento e Remediação Ativa)
Uma vez iniciado, o upgrade não deve ser deixado sem supervisão. A automação deve monitorar ativamente o progresso, principalmente dos MachineConfigPools (MCPs).
Lógica do "Listener":
A automação pode entrar em um loop de "poll" (verificar a cada X minutos) ou, de forma mais avançada com Event-Driven Ansible (EDA), reagir a eventos do OpenShift. Para uma implementação padrão no AAP, o polling é mais direto.
 * Iniciar o Upgrade: Use o shell ou command para executar o oc adm upgrade --to=....
 * Monitorar os MCPs (worker, infra, etc.):
   * Identificação: A automação deve periodicamente executar oc get mcp. O alvo é ver o UPDATED se tornar True para todos os pools. O principal sinal de problema é quando um MCP fica em UPDATING=True por tempo excessivo, ou DEGRADED=True.
   * Remediação Ativa:
     * Se um MCP estiver "pausado" ou demorando, o problema quase sempre é um nó que não consegue fazer o drain.
     * A automação deve:
       a. Identificar qual nó está sendo atualizado pelo MCP (oc describe mcp <mcp_name>).
       b. Executar os mesmos checks da Fase 1, mas especificamente no nó que está travado.
       c. Listar os pods nesse nó (oc get pods -o wide --field-selector spec.nodeName=<node_name>).
       d. Procurar por pods que estão impedindo o drain (geralmente aqueles sem controller, ou os mesmos problemas de PDB e Terminating).
       e. Aplicar as mesmas correções de forçar a remoção do pod problemático.
Este loop de monitoramento e correção é o "listener" ativo que você descreveu.
- name: Monitorar o progresso do upgrade dos MCPs
  ansible.builtin.shell: "oc get mcp {{ item }} -o jsonpath='{.status.conditions[?(@.type==\"Updated\")].status}'"
  register: mcp_status
  until: mcp_status.stdout == "True"
  retries: 60 # Tenta por 60 vezes
  delay: 60 # Espera 1 minuto entre tentativas (Total 1 hora)
  loop: "{{ ['worker', 'infra'] }}"
  # Se o loop falhar (timeout), aciona um playbook de remediação
  rescue:
    - name: O MCP não atualizou, iniciando diagnóstico
      include_role:
        name: remediate_stuck_mcp

Fase 3: Pós-Upgrade (Validação e Correção Final)
Após os MCPs e o Cluster Version indicarem a conclusão, é preciso validar a saúde do cluster e dos serviços.
 * Repetir os Pre-Flight Checks:
   * Verificar novamente os Cluster Operators.
   * Verificar se todos os nós estão Ready.
   * Procurar por pods em estados anômalos.
 * Corrigir Problemas Comuns Pós-Upgrade:
   * CSI Driver de Azure File e Dynatrace em CrashLoopBackOff: Este é um problema comum onde alguns pods de sistema não se recuperam bem. A solução mais simples e eficaz é uma reinicialização forçada.
   * Correção Automatizada:
     * Identifique os pods problemáticos (csi-azurefile-controller, oneagent, etc.) que estão em CrashLoopBackOff.
     * Execute um oc delete pod <pod_name> -n <namespace>. O Operator ou DaemonSet responsável irá recriá-los, o que geralmente resolve o problema, pois o novo pod é agendado com a configuração correta do cluster atualizado.
   - name: Encontrar pods de sistema em CrashLoopBackOff após upgrade
  k8s_info:
    kind: Pod
    label_selectors:
      - app=csi-azurefile-controller # Exemplo
      - app.kubernetes.io/name=dynatrace-oneagent # Exemplo
    field_selectors:
      - status.phase!=Running
  register: unhealthy_system_pods

- name: Reiniciar pods de sistema problemáticos
  k8s:
    state: absent
    api_version: v1
    kind: Pod
    name: "{{ item.metadata.name }}"
    namespace: "{{ item.metadata.namespace }}"
  loop: "{{ unhealthy_system_pods.resources }}"

 * Restaurar Configurações: Se você alterou PDBs na Fase 1, este é o momento de restaurá-los para seus valores originais.
Conclusão e Implementação no AAP
A melhor forma de implementar isso no Red Hat AAP é através de um Workflow Template:
 * Job Template 1: Pre-Flight Checks:
   * Executa o playbook 01_pre_flight.yml.
   * Em caso de sucesso, lança o Job Template 2.
   * Em caso de falha, envia uma notificação (Email, Slack, Teams) e para o processo.
 * Job Template 2: Execute & Monitor Upgrade:
   * Executa o playbook 02_run_upgrade.yml.
   * Contém a lógica de polling e remediação ativa.
   * Em caso de sucesso, lança o Job Template 3.
   * Em caso de falha (mesmo após tentativas de remediação), notifica e para.
 * Job Template 3: Post-Flight Validation:
   * Executa o playbook 03_post_flight.yml.
   * Faz a validação final e as correções necessárias.
   * Notifica o sucesso ou falha final da operação de upgrade completa.
Esta abordagem transforma sua automação de um simples script para um sistema robusto e inteligente, capaz de lidar com os problemas mais comuns de upgrades em massa de ARO, reduzindo drasticamente a necessidade de intervenção manual e garantindo a saúde dos clusters.
